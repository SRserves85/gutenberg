{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "from pyspark.sql.functions import (regexp_extract, regexp_replace, col, lower,\n",
    "                                   when, isnull, coalesce, udf, StringType)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads multiple text files\n",
    "rdd = sc.textFile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(x):\n",
    "    data = json.loads(x)\n",
    "    key = list(data.keys())[0]\n",
    "    dic = data[key]\n",
    "    \n",
    "    if len(dic['author']) > 0:\n",
    "        dic['author'] = dic['author'][0]\n",
    "    else:\n",
    "        dic['author'] = None\n",
    "        \n",
    "    if len(dic['language']) > 0:\n",
    "        dic['language'] = dic['language'][0]\n",
    "    else:\n",
    "        dic['language'] = None\n",
    "        \n",
    "    if len(dic['subject']) > 0:\n",
    "        dic['subject'] = dic['subject'][0]\n",
    "    else:\n",
    "        dic['subject'] = None\n",
    "        \n",
    "    if len(dic['title']) > 0:\n",
    "        dic['title'] = dic['title'][0]\n",
    "    else:\n",
    "        dic['title'] = None\n",
    "    \n",
    "    print(str(dic['txt'][0]))\n",
    "    dic['release_date'] = re.match(r\"(?<=Release Date:).*(?=(\\[E))\", str(dic['txt'][0]))\n",
    "    if dic['release_date'] == None:\n",
    "        dic['release_date'] = str(dic['txt'][0])\n",
    "        \n",
    "    \n",
    "        \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = rdd.map(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/2.2.1/libexec/python/pyspark/sql/session.py:356: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    }
   ],
   "source": [
    "# perform work on rdd\n",
    "df = rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out release date\n",
    "df = df.withColumn('release_date', regexp_extract(col('txt'), r\"(?<=Release Date:)(.*)(?=(\\[E))\", 1))\n",
    "\n",
    "# strip out newline characters\n",
    "df = df = df.withColumn('txt', regexp_replace(col('txt'), \"[\\n]\", \" \"))\n",
    "\n",
    "# lowercase the txt\n",
    "df = df.withColumn('txt', lower(col('txt')))\n",
    "\n",
    "# use regex to remove bad parts of the text gutenberg adds at the end\n",
    "df = df.withColumn('end', regexp_extract(col('txt'), r\"(.*)(?=end of the project gutenberg)\", 1))\n",
    "df = df.withColumn('end', when(df.end == \"\",\n",
    "                               regexp_extract(col('txt'), r\"(.*)(?=end of this project gutenberg)\", 1)).otherwise(df.end))\n",
    "\n",
    "# delete end column\n",
    "df = df.withColumn('txt', col('end'))\n",
    "df = df.drop('end')\n",
    "\n",
    "# use regex to remove bad parts of the start of gutenberg texts\n",
    "df = df.withColumn('start', regexp_extract(col('txt'), r\"(?<=start of this project gutenberg)(.*)\", 1))\n",
    "df = df.withColumn('start', when(df.start == \"\",\n",
    "                               regexp_extract(col('txt'), r\"(?<=start of the project gutenberg)(.*)\", 1)).otherwise(df.start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform operations\n",
    "df_p = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p['txt'][0][:8000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('txt', col('end'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
